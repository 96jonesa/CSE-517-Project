{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMGPaXrc5WJcM3hDFV7aWrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp0_axn_A0RP"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IouH5zC4C6tM"
      },
      "source": [
        "!pip3 install --quiet \"tensorflow-hub>=0.7.0\"\r\n",
        "!pip3 install --quiet seaborn\r\n",
        "!pip3 install --quiet pandas-market-calendars"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkbW7y5PAL4E"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F\r\n",
        "from absl import logging\r\n",
        "import tensorflow.compat.v1 as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import os\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import json\r\n",
        "import itertools\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import pandas_market_calendars as mcal\r\n",
        "import datetime\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3_bjmyzAy67"
      },
      "source": [
        "#Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_I8XFDzAQaW"
      },
      "source": [
        "class GRU(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\r\n",
        "        super(GRU, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "        self.batch_first = batch_first\r\n",
        "\r\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=self.batch_first)\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        output, hn = self.gru(input)\r\n",
        "        return output, hn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pc_MLjpAQsN"
      },
      "source": [
        "# attention weights are softmax(u^T tanh(W input + b)) where W is learned parameter matrix, u is a learned parameter vector, and b is a learned offset\r\n",
        "\r\n",
        "class LinearAttention(nn.Module):\r\n",
        "    def __init__(self, input_size, intermediate_size, weights_size):\r\n",
        "        super(LinearAttention, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "\r\n",
        "        self.linear_1 = nn.Linear(self.input_size, self.intermediate_size, bias=True)\r\n",
        "        self.linear_2 = nn.Linear(self.intermediate_size, self.weights_size, bias=False)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "        self.softmax = nn.Softmax(dim=2)\r\n",
        "\r\n",
        "    def forward(self, input, mask=None):\r\n",
        "        intermediate = self.tanh(self.linear_1(input))\r\n",
        "        pre_attention = self.linear_2(intermediate)\r\n",
        "        if mask is not None:\r\n",
        "            zero_vec = -9e15*torch.ones_like(pre_attention)\r\n",
        "            pre_attention = torch.where(mask > 0, pre_attention, zero_vec)\r\n",
        "        attention_weights = self.softmax(pre_attention)\r\n",
        "        attention_weights = attention_weights.permute(0, 2, 1)\r\n",
        "        output_features = torch.bmm(attention_weights, input)\r\n",
        "\r\n",
        "        return output_features"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZ4wJCUJATJ5"
      },
      "source": [
        "# output is ReLU(left^T W right + b) where W is a learned paramater matrix\r\n",
        "# and b is a learned bias\r\n",
        "\r\n",
        "class Blend(nn.Module):\r\n",
        "    def __init__(self, left_size, right_size, output_size):\r\n",
        "        super(Blend, self).__init__()\r\n",
        "        self.left_size = left_size\r\n",
        "        self.right_size = right_size\r\n",
        "        self.output_size = output_size\r\n",
        "\r\n",
        "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, output_size, bias=True)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "    def forward(self, left, right):\r\n",
        "        output = self.relu(self.bilinear(left, right))\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOSXvN33Ae-Q"
      },
      "source": [
        "# https://github.com/Diego999/pyGAT/blob/master/layers.py\r\n",
        "\r\n",
        "class SGAT(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, leakyrelu_slope=0.01):\r\n",
        "        super(SGAT, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        \r\n",
        "        self.W = nn.Parameter(torch.empty(size=(input_size, output_size)))\r\n",
        "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\r\n",
        "        self.a = nn.Parameter(torch.empty(size=(2*output_size, 1)))\r\n",
        "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\r\n",
        "        self.leakyrelu = nn.LeakyReLU(self.leakyrelu_slope)\r\n",
        "\r\n",
        "    def forward(self, h, adj):\r\n",
        "        Wh = torch.mm(h, self.W)\r\n",
        "        a_input = self._prepare_attentional_mechanism_input(Wh)\r\n",
        "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\r\n",
        "\r\n",
        "        zero_vec = -9e15*torch.ones_like(e)\r\n",
        "        attention = torch.where(adj > 0, e, zero_vec)\r\n",
        "        attention = F.softmax(attention, dim=1)\r\n",
        "        h_prime = torch.matmul(attention, Wh)\r\n",
        "\r\n",
        "        return h_prime\r\n",
        "\r\n",
        "    def _prepare_attentional_mechanism_input(self, Wh):\r\n",
        "        N = Wh.size()[0] # number of nodes\r\n",
        "        \r\n",
        "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\r\n",
        "        Wh_repeated_alternating = Wh.repeat(N, 1)\r\n",
        "\r\n",
        "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\r\n",
        "\r\n",
        "        return all_combinations_matrix.view(N, N, 2 * self.output_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgbnNp3nAh47"
      },
      "source": [
        "class MANSF(nn.Module):\r\n",
        "    def __init__(self, T, gru_hidden_size, attn_inter_size, use_embed_size,\r\n",
        "                 blend_size, gat_1_inter_size, gat_2_inter_size, leakyrelu_slope, elu_alpha, U):\r\n",
        "        super(MANSF, self).__init__()\r\n",
        "        self.T = T\r\n",
        "        self.gru_hidden_size = gru_hidden_size\r\n",
        "        self.attn_inter_size = attn_inter_size\r\n",
        "        self.use_embed_size = use_embed_size\r\n",
        "        self.blend_size = blend_size\r\n",
        "        self.gat_1_inter_size = gat_1_inter_size\r\n",
        "        self.gat_2_inter_size = gat_2_inter_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        self.elu_alpha = elu_alpha\r\n",
        "        self.U = U\r\n",
        "\r\n",
        "        self.gru_p = GRU(3, gru_hidden_size, batch_first=True)\r\n",
        "        self.gru_m = GRU(use_embed_size, gru_hidden_size, batch_first=True)\r\n",
        "        self.gru_s = GRU(gru_hidden_size, gru_hidden_size, batch_first=True)\r\n",
        "        self.attn_p = LinearAttention(gru_hidden_size, attn_inter_size, 1)\r\n",
        "        self.attn_m = LinearAttention(gru_hidden_size, attn_inter_size, 1)\r\n",
        "        self.attn_s = LinearAttention(gru_hidden_size, attn_inter_size, 1)\r\n",
        "        self.blend = Blend(gru_hidden_size, gru_hidden_size, blend_size)\r\n",
        "        self.mgat_1 = nn.ModuleList([SGAT(blend_size, gat_1_inter_size, leakyrelu_slope=leakyrelu_slope) for u in range(U)])\r\n",
        "        self.mgat_2 = nn.ModuleList([SGAT(U * gat_1_inter_size, gat_2_inter_size, leakyrelu_slope=leakyrelu_slope) for u in range(U)])\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "        self.elu = nn.ELU(elu_alpha)\r\n",
        "        self.final_linear = nn.Linear(U * gat_2_inter_size, 1, bias=True)\r\n",
        "\r\n",
        "    # p is price data tensor of shape (num_stocks, T, 3), for the day under consideration\r\n",
        "    # m is smi data list of tensors of shape (num_stocks, K, use_embed_size) of length T,\r\n",
        "    #       where K is the number of tweets for the given stock on the day under consideration\r\n",
        "    # neighorhoods is a list of adjacency lists, where each stock is indexed with the same\r\n",
        "    #       indices they have in p and m\r\n",
        "    def forward(self, p, m, m_mask, neighborhoods):\r\n",
        "        ## price encoding\r\n",
        "        h_p, _ = self.gru_p(p)\r\n",
        "        q = self.attn_p(h_p)\r\n",
        "\r\n",
        "        ## smi encoding (day level)\r\n",
        "        r = torch.zeros(p.shape[0], 0, self.gru_hidden_size)\r\n",
        "        r = r.to(device)\r\n",
        "        for t in range(self.T):\r\n",
        "            h_m, _ = self.gru_m(m[t])\r\n",
        "            r_t = self.attn_m(h_m, m_mask[t])\r\n",
        "            r = torch.cat((r, r_t), 1)\r\n",
        "\r\n",
        "        ## smi encoding (aggregate)\r\n",
        "        h_s, _ = self.gru_s(r)\r\n",
        "        c = self.attn_s(h_s)\r\n",
        "\r\n",
        "        ## blending\r\n",
        "        x = self.blend(q, c)\r\n",
        "\r\n",
        "        ## reshaping (eliminating superfluous dimension)\r\n",
        "        x = x.view(x.shape[0], x.shape[2])\r\n",
        "\r\n",
        "        ## first gat layer\r\n",
        "        #  first head\r\n",
        "        sgat = self.mgat_1[0]\r\n",
        "        z = sgat(x, neighborhoods)\r\n",
        "        z = self.elu(z)\r\n",
        "\r\n",
        "        #  remaining heads\r\n",
        "        for u in range(1, self.U):\r\n",
        "            sgat = self.mgat_1[u]\r\n",
        "            z_u = sgat(x, neighborhoods)\r\n",
        "            z_u = self.elu(z_u)\r\n",
        "            \r\n",
        "            z = torch.cat((z, z_u), 1)\r\n",
        "        \r\n",
        "        ## second gat layer\r\n",
        "        #  first head\r\n",
        "        sgat = self.mgat_2[0]\r\n",
        "        new_z = sgat(z, neighborhoods)\r\n",
        "        new_z = self.sigmoid(new_z)\r\n",
        "\r\n",
        "        #  remaining heads\r\n",
        "        for u in range(1, self.U):\r\n",
        "            sgat = self.mgat_2[u]\r\n",
        "            new_z_u = sgat(z, neighborhoods)\r\n",
        "            new_z_u = self.sigmoid(new_z_u)\r\n",
        "            \r\n",
        "            new_z = torch.cat((new_z, new_z_u), 1)\r\n",
        "        \r\n",
        "        ## final layer\r\n",
        "        y = self.sigmoid(self.final_linear(new_z))\r\n",
        "\r\n",
        "        ## return result\r\n",
        "        return y"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyyPL-psA4cU"
      },
      "source": [
        "#Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZhP9VnQCyZm"
      },
      "source": [
        "!wget https://github.com/yumoxu/stocknet-dataset/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aW75dPMQCzQ_"
      },
      "source": [
        "!unzip master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqxlViAXArti"
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRdIAHpeBJvw"
      },
      "source": [
        "tf.disable_v2_behavior()\r\n",
        "tf.compat.v1.disable_eager_execution()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1ab2p7BLDm"
      },
      "source": [
        "stocknet_dataset_filepath = './stocknet-dataset-master'\r\n",
        "train_start_date = '2014-01-01'\r\n",
        "train_end_date = '2015-07-31'\r\n",
        "val_start_date = '2015-08-01'\r\n",
        "val_end_date = '2015-09-30'\r\n",
        "test_start_date = '2015-10-01'\r\n",
        "test_end_date = '2016-01-01'"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIRBbhAZy9ls"
      },
      "source": [
        "def prep_dataset(dataset_filepath, start_date, end_date):\r\n",
        "    cache = {}\r\n",
        "    calendar = mcal.get_calendar('NYSE')\r\n",
        "    def next_trading_day(start_day=None, SAFE_DELTA = 4):\r\n",
        "        \"\"\"Returns the next/previous trading date separated by a certain number of \r\n",
        "        trading days.\r\n",
        "        \"\"\"\r\n",
        "        if start_day is None:\r\n",
        "            start_day = datetime.datetime.utcnow().date()\r\n",
        "        if start_day in cache:\r\n",
        "            return cache[start_day]\r\n",
        "        start = pd.to_datetime(start_day)\r\n",
        "        end = start + np.timedelta64(SAFE_DELTA, 'D')\r\n",
        "        business_days = calendar.valid_days(start_date=start, end_date=end)\r\n",
        "        next_day = business_days[1].date()\r\n",
        "        next_day = next_day.strftime(\"%Y-%m-%d\")\r\n",
        "        cache[start_day] = next_day\r\n",
        "        return next_day\r\n",
        "    \r\n",
        "    preprocessed_prices_filepath = stocknet_dataset_filepath + '/price/preprocessed'\r\n",
        "    preprocessed_tweets_filepath = stocknet_dataset_filepath + '/tweet/preprocessed'\r\n",
        "\r\n",
        "    company_to_price_df = {}\r\n",
        "    company_to_tweets = {}\r\n",
        "\r\n",
        "    for filename in os.listdir(preprocessed_prices_filepath):\r\n",
        "        with open(preprocessed_prices_filepath + '/' + filename) as file:\r\n",
        "            company_name = filename.split('.')[0]\r\n",
        "            \r\n",
        "            # Not enough data for GMRE\r\n",
        "            if company_name == 'GMRE':\r\n",
        "                continue\r\n",
        "            df = pd.read_csv(file, sep='\\t')\r\n",
        "            df.columns = ['date', 'open', 'high', 'low', 'close', 'adjust_close', 'volume']\r\n",
        "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\r\n",
        "            df = df.loc[mask]\r\n",
        "            company_to_price_df[company_name] = df.dropna()\r\n",
        "\r\n",
        "    for filename in tqdm(os.listdir(preprocessed_tweets_filepath)):\r\n",
        "        company_name = filename.split('.')[0]\r\n",
        "        dates_to_tweets = {}\r\n",
        "        for tweet_filename in os.listdir(preprocessed_tweets_filepath + '/' + filename):\r\n",
        "            if tweet_filename < start_date or tweet_filename > end_date:\r\n",
        "                continue\r\n",
        "            with open(preprocessed_tweets_filepath + '/' + filename + '/' + tweet_filename) as file:\r\n",
        "                list_of_tweets = []\r\n",
        "                for line in file:\r\n",
        "                    tweet_json = json.loads(line)\r\n",
        "                    list_of_tweets.append(tweet_json)\r\n",
        "                date_idx = next_trading_day(tweet_filename)\r\n",
        "                if date_idx not in dates_to_tweets:\r\n",
        "                    dates_to_tweets[date_idx] = list_of_tweets\r\n",
        "                else:\r\n",
        "                    dates_to_tweets[date_idx] += list_of_tweets\r\n",
        "        company_to_tweets[company_name] = dates_to_tweets\r\n",
        "    \r\n",
        "    # Reduce logging output.\r\n",
        "    logging.set_verbosity(logging.ERROR)\r\n",
        "    tf.get_logger().setLevel(logging.ERROR)\r\n",
        "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\r\n",
        "\r\n",
        "    # Import the Universal Sentence Encoder's TF Hub module\r\n",
        "    def embed_useT(module):\r\n",
        "        with tf.Graph().as_default():\r\n",
        "            sentences = tf.placeholder(tf.string)\r\n",
        "            embed = hub.Module(module)\r\n",
        "            embeddings = embed(sentences)\r\n",
        "            session = tf.train.MonitoredSession()\r\n",
        "        return lambda x: session.run(embeddings, {sentences: x})\r\n",
        "    embed_fn = embed_useT(module_url)\r\n",
        "\r\n",
        "    # Generate embeddings\r\n",
        "    for company in tqdm(company_to_tweets.keys()):\r\n",
        "        for date in company_to_tweets[company].keys():\r\n",
        "            messages = []\r\n",
        "            for j in range(len(company_to_tweets[company][date])):\r\n",
        "                messages.append(' '.join(company_to_tweets[company][date][j]['text']))\r\n",
        "                message_embeddings = embed_fn(messages)\r\n",
        "            for k in range(len(company_to_tweets[company][date])):\r\n",
        "                company_to_tweets[company][date][k]['embedding'] = list(message_embeddings[k])\r\n",
        "    \r\n",
        "    # Create date mapping\r\n",
        "    date_universe = set()\r\n",
        "    for company in company_to_price_df.keys():\r\n",
        "        date_universe = date_universe.union(set(company_to_price_df[company].date))\r\n",
        "    for company in company_to_tweets.keys():\r\n",
        "        date_universe = date_universe.union(set(company_to_tweets[company].keys()))\r\n",
        "    date_universe = sorted(list(date_universe))\r\n",
        "    index_to_date = {i-5:d for i,d in enumerate(date_universe)}\r\n",
        "    date_to_index = {d:i-5 for i,d in enumerate(date_universe)}\r\n",
        "\r\n",
        "    # Calculate dimensions for tensor\r\n",
        "    n_stocks = len(company_to_tweets.keys())\r\n",
        "    n_days = len(date_universe)\r\n",
        "    max_tweets = 0\r\n",
        "    for c,d in itertools.product(company_to_tweets.keys(), date_universe):\r\n",
        "        if d in company_to_tweets[c]:\r\n",
        "            max_tweets = max(max_tweets, len(company_to_tweets[c][d]))\r\n",
        "    # Create index mapping for stocks alphabetically\r\n",
        "    company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\r\n",
        "\r\n",
        "    return company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Td7nHz4py1wv"
      },
      "source": [
        "#Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XrZt8s1Kys5"
      },
      "source": [
        "class StockDataset(Dataset):\r\n",
        "    \"\"\"Price dataset\"\"\"\r\n",
        "\r\n",
        "    def __init__(self, company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets):\r\n",
        "        # Initialize class members\r\n",
        "        self.n_stocks = n_stocks\r\n",
        "        self.n_days = n_days\r\n",
        "        self.max_tweets = max_tweets\r\n",
        "\r\n",
        "        # Build maps\r\n",
        "        self.company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\r\n",
        "        self.date_to_index = {d:i for i,d in enumerate(date_universe)}\r\n",
        "        self.index_to_date = {i:d for i,d in enumerate(date_universe)}\r\n",
        "\r\n",
        "        # Store data\r\n",
        "        self.company_to_price_df = company_to_price_df\r\n",
        "        self.company_to_tweets = company_to_tweets\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.n_days-6\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        \"\"\"\r\n",
        "        gets a price tensor of shape (n_stocks, 6, 3)\r\n",
        "        gets a smi tensor of shape (n_stocks, 6, K, 512)\r\n",
        "        \"\"\"\r\n",
        "        # Size of sliding window\r\n",
        "        window = 6\r\n",
        "\r\n",
        "        if torch.is_tensor(idx):\r\n",
        "            idx = idx.tolist()\r\n",
        "\r\n",
        "        # Dates that we need to look up\r\n",
        "        dates_range = [self.index_to_date[i] for i in range(idx, idx + window)]\r\n",
        "\r\n",
        "        # Day after (for label)\r\n",
        "        day_after = self.index_to_date[idx + window]\r\n",
        "\r\n",
        "        # Which stocks are usable for these dates\r\n",
        "        usable_stocks = torch.ones(self.n_stocks)\r\n",
        "\r\n",
        "        # Labels\r\n",
        "        labels = torch.zeros(n_stocks)\r\n",
        "\r\n",
        "        # Get labels\r\n",
        "        for company in self.company_to_price_df.keys():\r\n",
        "            df = self.company_to_price_df[company]\r\n",
        "\r\n",
        "            # Grab row with particular date\r\n",
        "            row = df.loc[df['date'] == day_after]\r\n",
        "            c_index = self.company_to_index[company]\r\n",
        "\r\n",
        "            if (len(row['adjust_close']) > 0):\r\n",
        "                close = np.zeros((1))\r\n",
        "                close[0] = row['adjust_close']\r\n",
        "                if close > 0:\r\n",
        "                    labels[c_index] = 1\r\n",
        "            else:\r\n",
        "                usable_stocks[c_index] = 0\r\n",
        "\r\n",
        "        # Get price data tensor: n_stocks, window, 3\r\n",
        "        price_data = np.zeros((n_stocks, window, 3))\r\n",
        "        for company in self.company_to_price_df.keys():\r\n",
        "            df = self.company_to_price_df[company]\r\n",
        "\r\n",
        "            # Look up specific rows in DF\r\n",
        "            for date_idx, date in enumerate(dates_range):\r\n",
        "\r\n",
        "                # Grab row with particular date\r\n",
        "                row = df.loc[df['date'] == date]\r\n",
        "                c_index = self.company_to_index[company]\r\n",
        "\r\n",
        "                if (len(row['high']) > 0) and (len(row['low']) > 0) and (len(row['adjust_close']) > 0):\r\n",
        "                    price_data[c_index, date_idx, 0] = row['high']\r\n",
        "                    price_data[c_index, date_idx, 1] = row['low']\r\n",
        "                    price_data[c_index, date_idx, 2] = row['adjust_close']\r\n",
        "                else:\r\n",
        "                    usable_stocks[c_index] = 0\r\n",
        "\r\n",
        "        # Extract tweets for specific window\r\n",
        "        smi_data = np.zeros((n_stocks, window, max_tweets, 512))\r\n",
        "        tweet_counts = np.zeros((n_stocks, window))\r\n",
        "        for company in self.company_to_tweets.keys():\r\n",
        "\r\n",
        "            # Look up tweets from specific days\r\n",
        "            for date_idx, date in enumerate(dates_range):\r\n",
        "                n_tweets = 0\r\n",
        "                tweets = []\r\n",
        "                c_index = self.company_to_index[company]\r\n",
        "                if date in self.company_to_tweets[company]:\r\n",
        "                    n_tweets = len(self.company_to_tweets[company][date])\r\n",
        "                    tweets = [self.company_to_tweets[company][date][k]['embedding'] for k in range(n_tweets)]\r\n",
        "                else:\r\n",
        "                    usable_stocks[c_index] = 0\r\n",
        "                tweet_counts[c_index, date_idx] = n_tweets\r\n",
        "                if n_tweets == 0:\r\n",
        "                    usable_stocks[c_index] = 0\r\n",
        "                for i,embedding in enumerate(tweets): \r\n",
        "                    #stocks, day, lags, tweet, embedding\r\n",
        "                    smi_data[c_index, date_idx, i, :] = embedding[:]\r\n",
        "\r\n",
        "        usable_stocks = (usable_stocks == 1)\r\n",
        "\r\n",
        "        m_mask = torch.zeros(6, n_stocks, self.max_tweets, 1)\r\n",
        "        for t in range(6):\r\n",
        "            for i in range(n_stocks):\r\n",
        "                for k in range(self.max_tweets):\r\n",
        "                    if k <= tweet_counts[i][t]:\r\n",
        "                        m_mask[t][i][k][0] = 1\r\n",
        "\r\n",
        "        price_output = price_data[usable_stocks,:,:]\r\n",
        "        smi_output = smi_data[usable_stocks,:,:,:]\r\n",
        "        tweet_count = tweet_counts[usable_stocks,:]\r\n",
        "        m_mask = m_mask[:,usable_stocks,:,:]\r\n",
        "        labels = labels[usable_stocks]\r\n",
        "        \r\n",
        "        # construct output\r\n",
        "        return price_output, smi_output, tweet_count, usable_stocks, labels, m_mask"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VwHfOnq19zW"
      },
      "source": [
        "train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets = prep_dataset(stocknet_dataset_filepath, train_start_date, train_end_date)\r\n",
        "val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets = prep_dataset(stocknet_dataset_filepath, val_start_date, val_end_date)\r\n",
        "test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets = prep_dataset(stocknet_dataset_filepath, test_start_date, test_end_date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W_Y3YxELDp6"
      },
      "source": [
        "train_dataset = StockDataset(train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets)\r\n",
        "val_dataset = StockDataset(val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets)\r\n",
        "test_dataset = StockDataset(test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV_GvReJLFM5"
      },
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=1,\r\n",
        "                        shuffle=True, num_workers=0)\r\n",
        "\r\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1,\r\n",
        "                        shuffle=False, num_workers=0)\r\n",
        "\r\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1,\r\n",
        "                        shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlOeo8caLWKK"
      },
      "source": [
        "#Separator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANYU8fO-BdS6"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzGtCej6Bu_n"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etylmeyDBtQ4"
      },
      "source": [
        "mansf = MANSF(T=6,\r\n",
        "              gru_hidden_size=64,\r\n",
        "              attn_inter_size=32,\r\n",
        "              use_embed_size=512,\r\n",
        "              blend_size=32,\r\n",
        "              gat_1_inter_size=32,\r\n",
        "              gat_2_inter_size=32,\r\n",
        "              leakyrelu_slope=0.01,\r\n",
        "              elu_alpha=1.0,\r\n",
        "              U=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNy0SuSXB0H2"
      },
      "source": [
        "mansf = mansf.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELS9f6JKCDCU"
      },
      "source": [
        "optimizer = optim.Adam(mansf.parameters(), lr=5e-4)\r\n",
        "loss_fn = nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeuL7Yt_shSW"
      },
      "source": [
        "train_acc_list = []\r\n",
        "val_acc_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhxL1HqYCEPD"
      },
      "source": [
        "for epoch in range(18):\r\n",
        "    mansf.train()\r\n",
        "    correct = 0.0\r\n",
        "    total = 0.0\r\n",
        "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(train_dataloader):\r\n",
        "        price = price.type(torch.FloatTensor)\r\n",
        "        smi = smi.type(torch.FloatTensor)\r\n",
        "\r\n",
        "        price = price.to(device)\r\n",
        "        smi = smi.to(device)\r\n",
        "        n_tweets = n_tweets.to(device)\r\n",
        "        usable_stocks = usable_stocks.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        m_mask = m_mask.to(device)\r\n",
        "\r\n",
        "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\r\n",
        "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\r\n",
        "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\r\n",
        "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\r\n",
        "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\r\n",
        "\r\n",
        "        smi = smi.permute(1, 0, 2, 3)\r\n",
        "\r\n",
        "        m = []\r\n",
        "        for t in range(6):\r\n",
        "            m.append(smi[t])\r\n",
        "\r\n",
        "        neighborhoods = torch.eye(n_stocks, n_stocks)\r\n",
        "        neighborhoods = neighborhoods.to(device)\r\n",
        "        neighborhoods = neighborhoods[usable_stocks, usable_stocks]\r\n",
        "\r\n",
        "        if price.shape[0] != 0:\r\n",
        "            y = mansf(price, smi, m_mask, neighborhoods)\r\n",
        "            loss = loss_fn(y.view(-1), labels.view(-1))\r\n",
        "            loss.backward()\r\n",
        "            optimizer.step()\r\n",
        "            optimizer.zero_grad()\r\n",
        "            correct += torch.sum((y > 0.5) == labels).item()\r\n",
        "            total += len(price.shape[0])\r\n",
        "\r\n",
        "    train_acc = correct / total\r\n",
        "    train_acc_list.append(train_acc)\r\n",
        "\r\n",
        "    mansf.eval()\r\n",
        "    correct = 0.0\r\n",
        "    total = 0.0\r\n",
        "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(val_dataloader):\r\n",
        "        price = price.type(torch.FloatTensor)\r\n",
        "        smi = smi.type(torch.FloatTensor)\r\n",
        "\r\n",
        "        price = price.to(device)\r\n",
        "        smi = smi.to(device)\r\n",
        "        n_tweets = n_tweets.to(device)\r\n",
        "        usable_stocks = usable_stocks.to(device)\r\n",
        "        labels = labels.to(device)\r\n",
        "        m_mask = m_mask.to(device)\r\n",
        "\r\n",
        "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\r\n",
        "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\r\n",
        "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\r\n",
        "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\r\n",
        "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\r\n",
        "\r\n",
        "        smi = smi.permute(1, 0, 2, 3)\r\n",
        "\r\n",
        "        m = []\r\n",
        "        for t in range(6):\r\n",
        "            m.append(smi[t])\r\n",
        "\r\n",
        "        neighborhoods = torch.eye(n_stocks, n_stocks)\r\n",
        "        neighborhoods = neighborhoods.to(device)\r\n",
        "        neighborhoods = neighborhoods[usable_stocks, usable_stocks]\r\n",
        "\r\n",
        "        if price.shape[0] != 0:\r\n",
        "            y = mansf(price, smi, m_mask, neighborhoods)\r\n",
        "            correct += torch.sum((y > 0.5) == labels).item()\r\n",
        "            total += len(price.shape[0])\r\n",
        "\r\n",
        "    val_acc = correct / total\r\n",
        "    val_acc_list.append(val_acc)\r\n",
        "\r\n",
        "    print('epoch:', epoch, 'loss:', loss.item(), 'train_acc:', train_acc, 'val_acc:', val_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64uHrKiQrq4X"
      },
      "source": [
        "#Figures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGV8hlffrr9k"
      },
      "source": [
        "def plot(X, Y, xlabel, ylabel, legend, title):\r\n",
        "    fig = plt.figure()\r\n",
        "    ax = fig.add_subplot(1, 1, 1)\r\n",
        "\r\n",
        "    for i in range(len(Y)):\r\n",
        "        ax.plot(X, Y[i], label=legend[i])\r\n",
        "\r\n",
        "    plt.grid(color='0.95')\r\n",
        "    plt.legend()\r\n",
        "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6pHZ4GIr1BI"
      },
      "source": [
        "plot(range(5),\r\n",
        "     [train_acc_list, val_acc_list],\r\n",
        "     'epoch',\r\n",
        "     'accuracy',\r\n",
        "     ['training accuracy', 'validation accuracy'],\r\n",
        "     'accuracy vs. epoch')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}