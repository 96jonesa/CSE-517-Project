{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scaffolding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM83NCBoJefT4UN0LqX83zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/scaffolding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUxhN7LDSGGz"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bn0eEsGSB3N"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IchYXapSJjH"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-YF6GyHzwE2"
      },
      "source": [
        "##GRU\r\n",
        "This is just a wrapper around nn.GRU for the sake of consistency. Used in the Price Encoder, day-level SMI Encoder, and temporal SMI Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0OVZXsnSJH3"
      },
      "source": [
        "class GRU(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(GRU, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        self.gru = nn.GRU(input_size, hidden_size)\r\n",
        "\r\n",
        "    def forward(self, input, h_0):\r\n",
        "        output, hn = self.gru(input, h_0)\r\n",
        "        return output, hn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRniLDIGSf5Y"
      },
      "source": [
        "#Self-Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDM1G23a0J4V"
      },
      "source": [
        "##LinearAttention\r\n",
        "The attention mechanism used in Feng et. al. Used in the Price Encoder, day-level SMI Encoder, and temporal SMI Encoder. Given input $h$, returns\r\n",
        "$q_t = \\sum_{i=t-T}^T \\beta_i h_i$ where $\\beta_i = \\dfrac{\\exp\\left( u^T \\tanh \\left( W h_i + b \\right) \\right)}{\\sum_{k=t-T}^t \\exp\\left( u^T \\tanh \\left( W h_k + b \\right) \\right)}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDJ054vAquLa"
      },
      "source": [
        "# attention weights are softmax(u^T tanh(W input + b)) where W is learned parameter matrix, u is a learned parameter vector, and b is a learned offset\r\n",
        "\r\n",
        "class LinearAttention(nn.Module):\r\n",
        "    def __init__(self, input_size, intermediate_size, weights_size):\r\n",
        "        super(LinearAttention, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "\r\n",
        "        self.linear_1 = nn.Linear(self.input_size, self.intermediate_size, bias=True)\r\n",
        "        self.linear_2 = nn.Linear(self.intermediate_size, self.weights_size, bias=False)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        intermediate = self.tanh(self.linear_1(input))\r\n",
        "        attention_weights = self.softmax(self.linear_2(intermediate))\r\n",
        "        output_features = torch.mm(attention_weights, input)  # check this\r\n",
        "\r\n",
        "        return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHtjY2hP0XmG"
      },
      "source": [
        "##BilinearAttention\r\n",
        "The attention mechanism proposed in Sawhney et. al, which does not work with inputs of the shapes proposed in the paper. Various choices of left and right vectors could be used here to provide an alternative to the LinearAttention module. Given inputs $L$ and $R$, returns\r\n",
        "$q_t = \\sum_{i=t-T}^T \\beta_i h_i$ where $\\beta_i = \\dfrac{\\exp\\left( L_i^T W R  \\right)}{\\sum_{k=t-T}^t \\exp\\left( L_k^T W R \\right)}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diqacMm-Sfjs"
      },
      "source": [
        "# attention weights are softmax(left^T W right) where W is learned parameter matrix\r\n",
        "\r\n",
        "class BilinearAttention(nn.Module):\r\n",
        "    def __init__(self, left_size, right_size, weights_size):\r\n",
        "        super(BilinearAttention, self).__init__()\r\n",
        "        self.left_size = left_size\r\n",
        "        self.right_size = right_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "\r\n",
        "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, weights_size, bias=False)\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, left, right):\r\n",
        "        attention_weights = self.softmax(self.bilinear(left, right))\r\n",
        "        output_features = torch.mm(attention_weights, left)  # check this\r\n",
        "\r\n",
        "        return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qkvwOlPW3kt"
      },
      "source": [
        "#Blending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AMUuaLR06Ph"
      },
      "source": [
        "##Blend\r\n",
        "Applies a learned bilinear transformation to the left and right vectors, then inputs the result to a ReLU non-linearity. Used to obtain Multi-Modal Encodings from Price Encodings and temporal SMI Encodings. Given Price Encodings $q_t$ and temporal SMI Encodings $c_t$, returns\r\n",
        "$x_t = \\mathcal{B} \\left( c_t, q_t \\right) = \\text{ReLU} \\left( q_t^T W c_t + b \\right)$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmdq6ozMV1u8"
      },
      "source": [
        "# output is ReLU(left^T W right + b) where W is a learned paramater matrix\r\n",
        "# and b is a learned bias\r\n",
        "\r\n",
        "class Blend(nn.Module):\r\n",
        "    def __init__(self, left_size, right_size, output_size):\r\n",
        "        super(Blend, self).__init__()\r\n",
        "        self.left_size = left_size\r\n",
        "        self.right_size = right_size\r\n",
        "        self.output_size = output_size\r\n",
        "\r\n",
        "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, output_size, bias=True)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "    def forward(self, left, right):\r\n",
        "        output = self.relu(self.bilinear(left, right))\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zknLj7UautA"
      },
      "source": [
        "#Single-Headed Graph Attention Network (SGAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3MQfBWhE1XR6"
      },
      "source": [
        "##SharedLinear\r\n",
        "This is just a wrapper around nn.Linear for the sake of consistency. Used to apply a shared linear transformation to all inputs of an SGAT layer. Under current implementation, this should be applied before passing inputs to SGAT."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DYXTg2piK_J"
      },
      "source": [
        "# need shared learned parameter matrix W to multiply against each input vector\r\n",
        "\r\n",
        "class SharedLinear(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size):\r\n",
        "        super(SharedLinear, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        \r\n",
        "        self.linear = nn.Linear(input_size, output_size, bias=False)\r\n",
        "    \r\n",
        "    def forward(self, input):\r\n",
        "        output = self.linear(input)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iu7zaUMD10iy"
      },
      "source": [
        "##SGAT\r\n",
        "A single-headed GAT layer. A shared linear transform $W$ is applied to all the nodes *before* passing them as input to this module (by passing them as input to a SharedLinear layer), then a shared self-attention mechanism is applied to each node $i$ in its immediate neighborhood $\\mathcal{N}_i$. For each node $j\\in \\mathcal{N}_i$, normalized attention coefficients $\\alpha_{i,j}$ are computed to represent the importance of the relations between stocks $i$ and $j$. That is,\r\n",
        "$\\alpha_{i,j} = \\dfrac{\\exp ( \\text{LeakyReLU} ( a_w^T [ W x_i \r\n",
        "\\oplus W x_j ] ) )}{\\sum_{k\\in \\mathcal{N}_i} \\exp ( \\text{LeakyReLU} ( a_w^T [ W x_i \\oplus W x_k ] ) )}$\r\n",
        "where $\\oplus$ denotes concatenation and $a_w$ is a learned parameter matrix. An updated feature vector $z_i$ for the $i$-th stock is computed by applying these attention weights to the linearly transformed multi-modal feature vectors of all of the stocks in $\\mathcal{N}_i$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngyqy4cBas1u"
      },
      "source": [
        "# merge code with MGAT code to form general case GAT code\r\n",
        "\r\n",
        "class SGAT(nn.Module):\r\n",
        "    def __init__(self, input_size, weights_size, leakyrelu_slope):\r\n",
        "        super(SGAT, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        \r\n",
        "        self.linear = nn.Linear(2 * input_size, weights_size, bias=False)\r\n",
        "        self.leakyrelu = nn.LeakyReLU(self.leakyrelu_slope)\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, input, neighborhoods):\r\n",
        "        attention_weights = self.softmax(self.leakyrelu(self.linear(torch.cat(input, neighborhoods))))  # check this\r\n",
        "        output_features = torch.mm(attention_weights, input)  # check this\r\n",
        "\r\n",
        "        return output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Vx-ttvbUG_"
      },
      "source": [
        "#Multi-Headed Graph Attention Network (MGAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QNC0S6jQDk"
      },
      "source": [
        "# decide between implementing like this and implementing in main module\r\n",
        "\r\n",
        "class MGAT(nn.Module):\r\n",
        "    def __init__(self, input_size, weights_size, leakyrelu_slope):\r\n",
        "        super(MGAT, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        \r\n",
        "        self.sgat = SGAT(self.input_size, self.weights_size, self.leakyrelu_slope)\r\n",
        "\r\n",
        "    def forward(self, input, neighborhoods, num_heads):\r\n",
        "        attention_weights =  # initialize to correct shape\r\n",
        "        output_features =  # initialize to correct shape\r\n",
        "        for i in range(num_heads):\r\n",
        "            attention_weights[i], output_features[[i]] = self.sgat(input, neighborhoods)  # should we initialize fresh SGAT?\r\n",
        "\r\n",
        "        return output_features  # check shape of output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8lDrqXkPIR"
      },
      "source": [
        "#Scaffolding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwruge4tkOzx"
      },
      "source": [
        "class MANSF(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, leakyrelu_slope, elu_alpha):\r\n",
        "        super(MANSF, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        self.elu_alpha = elu_alpha\r\n",
        "\r\n",
        "        # fill in parameters for all of these\r\n",
        "        self.gru_p = GRU()\r\n",
        "        self.gru_m = GRU()\r\n",
        "        self.gru_s = GRU()\r\n",
        "        self.attn_p = LinearAttention()\r\n",
        "        self.attn_m = LinearAttention()\r\n",
        "        self.attn_s = LinearAttention()\r\n",
        "        self.sgat_1 = SGAT()\r\n",
        "        self.sgat_2 = SGAT()\r\n",
        "        self.blend = Blend()\r\n",
        "        self.elu = nn.ELU(elu_alpha)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "        self.linear = nn.Linear()\r\n",
        "\r\n",
        "    def forward(self, price_input, smi_input, neighborhoods, num_heads):\r\n",
        "        _, price_gru_states = self.gru_p(price_input)\r\n",
        "        price_encoding = self.attn_p(price_gru_states)\r\n",
        "\r\n",
        "        _, smi_day_gru_states = self.gru_m(smi_input)\r\n",
        "        smi_day_encoding = self.attn_m(smi_day_gru_states)\r\n",
        "        _, smi_gru_states = self.gru_s(smi_day_encoding)\r\n",
        "        smi_encoding = self.attn_s(smi_gru_states)\r\n",
        "\r\n",
        "        multi_modal_encoding = self.blend(price_encoding, smi_encoding)\r\n",
        "\r\n",
        "        z_1 = self.elu(self.sgat_1(multi_modal_encoding, neighborhoods))\r\n",
        "        z_2 = self.sigmoid(self.sgat_2(z_1, neighborhoods))\r\n",
        "        y = self.sigmoid(self.linear(z_2))\r\n",
        "\r\n",
        "        return y"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}