{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scaffolding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMJJ6kbb5uDNb7MHKeayVUh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/scaffolding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUxhN7LDSGGz"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bn0eEsGSB3N"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IchYXapSJjH"
      },
      "source": [
        "#GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0OVZXsnSJH3"
      },
      "source": [
        "class GRU(nn.Module):\r\n",
        "    def __init__(self, input_size, hidden_size):\r\n",
        "        super(GRU, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.hidden_size = hidden_size\r\n",
        "\r\n",
        "        self.gru = nn.GRU(input_size, hidden_size)\r\n",
        "\r\n",
        "    def forward(self, input, h_0):\r\n",
        "        output, hn = self.gru(input, h_0)\r\n",
        "        return output, hn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRniLDIGSf5Y"
      },
      "source": [
        "#Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diqacMm-Sfjs"
      },
      "source": [
        "# attention weights are softmax(left^T W right) where W is learned parameter matrix\r\n",
        "\r\n",
        "class BilinearAttention(nn.Module):\r\n",
        "    def __init__(self, left_size, right_size, weights_size):\r\n",
        "        super(BilinearAttention, self).__init__()\r\n",
        "        self.left_size = left_size\r\n",
        "        self.right_size = right_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "\r\n",
        "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, weights_size, bias=False)\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, left, right):\r\n",
        "        attention_weights = self.softmax(self.bilinear(left, right))\r\n",
        "        output_features = torch.mm(attention_weights, left)  # check this\r\n",
        "\r\n",
        "        return attention_weights, output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDJ054vAquLa"
      },
      "source": [
        "# attention weights are softmax(u^T tanh(W input + b)) where W is learned parameter matrix, u is a learned parameter vector, and b is a learned offset\r\n",
        "\r\n",
        "class LinearAttention(nn.Module):\r\n",
        "    def __init__(self, input_size, intermediate_size, weights_size):\r\n",
        "        super(LinearAttention, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.intermediate_size = intermediate_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "\r\n",
        "        self.linear_1 = nn.Linear(self.input_size, self.intermediate_size, bias=True)\r\n",
        "        self.linear_2 = nn.Linear(self.intermediate_size, self.weights_size, bias=False)\r\n",
        "        self.tanh = nn.Tanh()\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, input):\r\n",
        "        intermediate = self.tanh(self.linear_1(input))\r\n",
        "        attention_weights = self.softmax(self.linear_2(intermediate))\r\n",
        "        output_features = torch.mm(attention_weights, input)  # check this\r\n",
        "\r\n",
        "        return attention_weights, output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qkvwOlPW3kt"
      },
      "source": [
        "#Blending"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmdq6ozMV1u8"
      },
      "source": [
        "# output is ReLU(left^T W right + b) where W is a learned paramater matrix\r\n",
        "# and b is a learned bias\r\n",
        "\r\n",
        "class Blend(nn.Module):\r\n",
        "    def __init__(self, left_size, right_size, output_size):\r\n",
        "        super(Blend, self).__init__()\r\n",
        "        self.left_size = left_size\r\n",
        "        self.right_size = right_size\r\n",
        "        self.output_size = output_size\r\n",
        "\r\n",
        "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, output_size, bias=True)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "    \r\n",
        "    def forward(self, left, right):\r\n",
        "        output = self.relu(self.bilinear(left, right))\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zknLj7UautA"
      },
      "source": [
        "#Single-Headed Graph Attention Network (SGAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DYXTg2piK_J"
      },
      "source": [
        "# need shared learned parameter matrix W to multiply against each input vector\r\n",
        "\r\n",
        "class SharedLinear(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size):\r\n",
        "        super(SharedLinear, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        \r\n",
        "        self.linear = nn.Linear(input_size, output_size, bias=False)\r\n",
        "    \r\n",
        "    def forward(self, input):\r\n",
        "        output = self.linear(input)\r\n",
        "\r\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngyqy4cBas1u"
      },
      "source": [
        "# merge code with MGAT code to form general case GAT code\r\n",
        "\r\n",
        "class SGAT(nn.Module):\r\n",
        "    def __init__(self, input_size, weights_size, leakyrelu_slope):\r\n",
        "        super(SGAT, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        \r\n",
        "        self.linear = nn.Linear(2 * input_size, weights_size, bias=False)\r\n",
        "        self.leakyrelu = nn.LeakyReLU(self.leakyrelu_slope)\r\n",
        "        self.softmax = nn.Softmax()\r\n",
        "\r\n",
        "    def forward(self, input, neighborhood):\r\n",
        "        attention_weights = self.softmax(self.leakyrelu(self.linear(torch.cat(input, neighborhood))))  # check concatenation axis\r\n",
        "        output_features = torch.mm(attention_weights, input)  # check this\r\n",
        "\r\n",
        "        return attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Vx-ttvbUG_"
      },
      "source": [
        "#Multi-Headed Graph Attention Network (MGAT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70QNC0S6jQDk"
      },
      "source": [
        "# decide between implementing like this and implementing in main module\r\n",
        "\r\n",
        "class MGAT(nn.Module):\r\n",
        "    def __init__(self, input_size, weights_size, leakyrelu_slope):\r\n",
        "        super(MGAT, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.weights_size = weights_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        \r\n",
        "        self.sgat = SGAT(self.input_size, self.weights_size, self.leakyrelu_slope)\r\n",
        "\r\n",
        "    def forward(self, input, neighborhood, num_heads):\r\n",
        "        attention_weights =  # initialize to correct shape\r\n",
        "        output_features =  # initialize to correct shape\r\n",
        "        for i in range(num_heads):\r\n",
        "            attention_weights[i], output_features[[i]] = self.sgat(input, neighborhood)  # should we initialize fresh SGAT?\r\n",
        "\r\n",
        "        return attention_weights, output_features  # check shape of output_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "km8lDrqXkPIR"
      },
      "source": [
        "#Scaffolding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwruge4tkOzx"
      },
      "source": [
        "class MANSF(nn.Module):\r\n",
        "    def __init__(self, input_size, output_size, leakyrelu_slope, elu_alpha):\r\n",
        "        super(MANSF, self).__init__()\r\n",
        "        self.input_size = input_size\r\n",
        "        self.output_size = output_size\r\n",
        "        self.leakyrelu_slope = leakyrelu_slope\r\n",
        "        self.elu_alpha = elu_alpha\r\n",
        "\r\n",
        "        # fill in parameters\r\n",
        "        self.gru_p = GRU()\r\n",
        "        self.gru_m = GRU()\r\n",
        "        self.gru_s = GRU()\r\n",
        "        self.attn_p = SelfAttention()\r\n",
        "        self.attn_m = SelfAttention()\r\n",
        "        self.attn_s = SelfAttention()\r\n",
        "        self.blend = Blend()\r\n",
        "        self.elu = nn.ELU(elu_alpha)\r\n",
        "        self.sigmoid = nn.Sigmoid()\r\n",
        "\r\n",
        "    def forward(self, input, neighborhoods, num_heads):\r\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}