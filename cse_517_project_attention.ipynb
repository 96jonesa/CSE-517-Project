{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cse_517_project_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOxKOy6PdPvhGLV0KjJNJDz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/cse_517_project_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h33wbv2CqGIO"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4CmMfjfp3aO"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from torch import optim\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QczBU6aFrk-L"
      },
      "source": [
        "#Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jmrh5OlirjOO"
      },
      "source": [
        "# gru_output[i]         the output of the GRU on day i, and has size ???\r\n",
        "# gru_hidden            the concatenation of the hidden states of the GRU, and has size (T, d_m) for prices and (K, d_m) for tweets\r\n",
        "#                       where T is the number of days in the lookback window, K is the number of tweets on the given day,\r\n",
        "#                       and d_m is the dimension of each hidden state\r\n",
        "# W                     learned parameter matrix\r\n",
        "# attention_weights[i]  should be exp(gru_output[i].T W gru_hidden) / sum(exp(gru_output[i].T W gru_hidden))\r\n",
        "# output_features       should be sum(attention_weights[i] gru_output[i])\r\n",
        "class Attention(nn.module):\r\n",
        "    def __init__(self, gru_output_size, gru_hidden_size, weights_size):\r\n",
        "        super(Attention, self).__init__()\r\n",
        "        self.gru_output_size = gru_output_size\r\n",
        "        self.gru_hidden_size = gru_hidden_size\r\n",
        "        self.output_size = output_size  # should this be used somewhere ?\r\n",
        "        self.weights_size = weights_size  # what should this be ?\r\n",
        "\r\n",
        "        self.fc = nn.Linear(self.gru_output_size, self.weights_size, bias=False)  # should bias=True? what should weights_size be ?\r\n",
        "\r\n",
        "    def forward(self, gru_output, gru_hidden):\r\n",
        "        attention_weights = F.softmax(torch.mm(self.fc(gru_output), gru_hidden))  # check shape (might need to reshape and/or transpose)\r\n",
        "        output_features = torch.mm(attention_weights, gru_output)  # check shape (might need to reshape and/or transpose)\r\n",
        "        return output_features, attention_weights  # what should this return ?"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}