{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp0_axn_A0RP"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IouH5zC4C6tM"
   },
   "outputs": [],
   "source": [
    "!pip3 install --quiet \"tensorflow-hub>=0.7.0\"\n",
    "!pip3 install --quiet seaborn\n",
    "!pip3 install --quiet pandas-market-calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hkbW7y5PAL4E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from absl import logging\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pandas_market_calendars as mcal\n",
    "import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3_bjmyzAy67"
   },
   "source": [
    "#Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "V_I8XFDzAQaW"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=self.batch_first)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hn = self.gru(input)\n",
    "        return output, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is ReLU(left^T W right + b) where W is a learned paramater matrix\n",
    "# and b is a learned bias\n",
    "\n",
    "class Blend(nn.Module):\n",
    "    def __init__(self, left_size, right_size, output_size):\n",
    "        super(Blend, self).__init__()\n",
    "        self.left_size = left_size\n",
    "        self.right_size = right_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, output_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, left, right):\n",
    "        output = self.relu(self.bilinear(left, right))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SgbnNp3nAh47"
   },
   "outputs": [],
   "source": [
    "class MANSF(nn.Module):\n",
    "    def __init__(self, T, gru_hidden_size, attn_inter_size, use_embed_size,\n",
    "                 blend_size, gat_1_inter_size, gat_2_inter_size, leakyrelu_slope, elu_alpha, U):\n",
    "        super(MANSF, self).__init__()\n",
    "        self.T = T\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.attn_inter_size = attn_inter_size\n",
    "        self.use_embed_size = use_embed_size\n",
    "        self.blend_size = blend_size\n",
    "        self.leakyrelu_slope = leakyrelu_slope\n",
    "        self.elu_alpha = elu_alpha\n",
    "        self.U = U\n",
    "\n",
    "        self.gru_p = GRU(3, gru_hidden_size, batch_first=True)\n",
    "        self.gru_m = GRU(use_embed_size, gru_hidden_size, batch_first=True)\n",
    "        self.gru_s = GRU(gru_hidden_size, gru_hidden_size, batch_first=True)\n",
    "        self.blend = Blend(gru_hidden_size, gru_hidden_size, blend_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.elu = nn.ELU(elu_alpha)\n",
    "        self.final_linear = nn.Linear(blend_size, 1, bias=True)\n",
    "\n",
    "    # p is price data tensor of shape (num_stocks, T, 3), for the day under consideration\n",
    "    # m is smi data list of tensors of shape (num_stocks, K, use_embed_size) of length T,\n",
    "    #       where K is the number of tweets for the given stock on the day under consideration\n",
    "    # neighorhoods is a list of adjacency lists, where each stock is indexed with the same\n",
    "    #       indices they have in p and m\n",
    "    def forward(self, p, m, m_mask, neighborhoods):\n",
    "        ## price encoding\n",
    "        _, h_p = self.gru_p(p)\n",
    "\n",
    "        ## smi encoding (day level)\n",
    "        r = torch.zeros(p.shape[0], 0, self.gru_hidden_size)\n",
    "        r = r.to(device)\n",
    "        for t in range(self.T):\n",
    "            h_m, _ = self.gru_m(m[t])\n",
    "            r = torch.cat((r, h_m), 1)\n",
    "\n",
    "        ## smi encoding (aggregate)\n",
    "        _, h_s = self.gru_s(r)\n",
    "\n",
    "        ## blending\n",
    "        x = self.blend(h_p, h_s)\n",
    "\n",
    "        ## reshaping (eliminating superfluous dimension)\n",
    "        x = x.squeeze()\n",
    "        \n",
    "        ## final layer\n",
    "        y = self.sigmoid(self.final_linear(x))\n",
    "\n",
    "        ## return result\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyyPL-psA4cU"
   },
   "source": [
    "#Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "JZhP9VnQCyZm"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/yumoxu/stocknet-dataset/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aW75dPMQCzQ_"
   },
   "outputs": [],
   "source": [
    "#!unzip master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xqxlViAXArti"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DRdIAHpeBJvw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/kxiao36/anaconda3/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "uK1ab2p7BLDm"
   },
   "outputs": [],
   "source": [
    "stocknet_dataset_filepath = './stocknet-dataset-master'\n",
    "train_start_date = '2014-01-01'\n",
    "train_end_date = '2015-07-31'\n",
    "val_start_date = '2015-08-01'\n",
    "val_end_date = '2015-09-30'\n",
    "test_start_date = '2015-10-01'\n",
    "test_end_date = '2016-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oIRBbhAZy9ls"
   },
   "outputs": [],
   "source": [
    "def prep_dataset(dataset_filepath, start_date, end_date):\n",
    "    cache = {}\n",
    "    calendar = mcal.get_calendar('NYSE')\n",
    "    def next_trading_day(start_day=None, SAFE_DELTA = 4):\n",
    "        \"\"\"Returns the next/previous trading date separated by a certain number of \n",
    "        trading days.\n",
    "        \"\"\"\n",
    "        if start_day is None:\n",
    "            start_day = datetime.datetime.utcnow().date()\n",
    "        if start_day in cache:\n",
    "            return cache[start_day]\n",
    "        start = pd.to_datetime(start_day)\n",
    "        end = start + np.timedelta64(SAFE_DELTA, 'D')\n",
    "        business_days = calendar.valid_days(start_date=start, end_date=end)\n",
    "        next_day = business_days[1].date()\n",
    "        next_day = next_day.strftime(\"%Y-%m-%d\")\n",
    "        cache[start_day] = next_day\n",
    "        return next_day\n",
    "    \n",
    "    raw_prices_filepath = stocknet_dataset_filepath + '/price/raw'\n",
    "    preprocessed_tweets_filepath = stocknet_dataset_filepath + '/tweet/preprocessed'\n",
    "\n",
    "    company_to_price_df = {}\n",
    "    company_to_tweets = {}\n",
    "\n",
    "    for filename in os.listdir(raw_prices_filepath):\n",
    "        with open(raw_prices_filepath + '/' + filename) as file:\n",
    "            company_name = filename.split('.')[0]\n",
    "            \n",
    "            # Not enough data for GMRE\n",
    "            if company_name == 'GMRE':\n",
    "                continue\n",
    "            df = pd.read_csv(file)\n",
    "            df.columns = ['date', 'open', 'high', 'low', 'close', 'adjust_close', 'volume']\n",
    "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "            df = df.loc[mask]\n",
    "            company_to_price_df[company_name] = df.dropna()\n",
    "\n",
    "    for filename in tqdm(os.listdir(preprocessed_tweets_filepath)):\n",
    "        company_name = filename.split('.')[0]\n",
    "        dates_to_tweets = {}\n",
    "        for tweet_filename in os.listdir(preprocessed_tweets_filepath + '/' + filename):\n",
    "            if tweet_filename < start_date or tweet_filename > end_date:\n",
    "                continue\n",
    "            with open(preprocessed_tweets_filepath + '/' + filename + '/' + tweet_filename) as file:\n",
    "                list_of_tweets = []\n",
    "                for line in file:\n",
    "                    tweet_json = json.loads(line)\n",
    "                    list_of_tweets.append(tweet_json)\n",
    "                date_idx = next_trading_day(tweet_filename)\n",
    "                if date_idx not in dates_to_tweets:\n",
    "                    dates_to_tweets[date_idx] = list_of_tweets\n",
    "                else:\n",
    "                    dates_to_tweets[date_idx] += list_of_tweets\n",
    "        company_to_tweets[company_name] = dates_to_tweets\n",
    "    \n",
    "    # Reduce logging output.\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "    tf.get_logger().setLevel(logging.ERROR)\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    def embed_useT(module):\n",
    "        with tf.Graph().as_default():\n",
    "            sentences = tf.placeholder(tf.string)\n",
    "            embed = hub.Module(module)\n",
    "            embeddings = embed(sentences)\n",
    "            session = tf.train.MonitoredSession()\n",
    "        return lambda x: session.run(embeddings, {sentences: x})\n",
    "    embed_fn = embed_useT(module_url)\n",
    "\n",
    "    # Generate embeddings\n",
    "    for company in tqdm(company_to_tweets.keys()):\n",
    "        for date in company_to_tweets[company].keys():\n",
    "            messages = []\n",
    "            for j in range(len(company_to_tweets[company][date])):\n",
    "                messages.append(' '.join(company_to_tweets[company][date][j]['text']))\n",
    "                message_embeddings = embed_fn(messages)\n",
    "            for k in range(len(company_to_tweets[company][date])):\n",
    "                company_to_tweets[company][date][k]['embedding'] = list(message_embeddings[k])\n",
    "    \n",
    "    # Create date mapping\n",
    "    date_universe = set()\n",
    "    for company in company_to_price_df.keys():\n",
    "        date_universe = date_universe.union(set(company_to_price_df[company].date))\n",
    "    for company in company_to_tweets.keys():\n",
    "        date_universe = date_universe.union(set(company_to_tweets[company].keys()))\n",
    "    date_universe = sorted(list(date_universe))\n",
    "    index_to_date = {i-5:d for i,d in enumerate(date_universe)}\n",
    "    date_to_index = {d:i-5 for i,d in enumerate(date_universe)}\n",
    "\n",
    "    # Calculate dimensions for tensor\n",
    "    n_stocks = len(company_to_tweets.keys())\n",
    "    n_days = len(date_universe)\n",
    "    max_tweets = 0\n",
    "    for c,d in itertools.product(company_to_tweets.keys(), date_universe):\n",
    "        if d in company_to_tweets[c]:\n",
    "            max_tweets = max(max_tweets, len(company_to_tweets[c][d]))\n",
    "    # Create index mapping for stocks alphabetically\n",
    "    company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\n",
    "\n",
    "    return company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7VwHfOnq19zW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd222f17c55c449893f117d701f8c012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba602478c92431b937733ebc79b748c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0da78ed324fb4626bd132855d13fdd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c317f8ea2b42eda6bfaa4e7a7860b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66cd106ee2174a4e8688a43d5309c479",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b718590c2ec4772be544073810d303d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets = prep_dataset(stocknet_dataset_filepath, train_start_date, train_end_date)\n",
    "val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets = prep_dataset(stocknet_dataset_filepath, val_start_date, val_end_date)\n",
    "test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets = prep_dataset(stocknet_dataset_filepath, test_start_date, test_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td7nHz4py1wv"
   },
   "source": [
    "#Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "8XrZt8s1Kys5"
   },
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \"\"\"Price dataset\"\"\"\n",
    "\n",
    "    def __init__(self, company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets):\n",
    "        # Initialize class members\n",
    "        self.n_stocks = n_stocks\n",
    "        self.n_days = n_days\n",
    "        self.max_tweets = max_tweets\n",
    "        self.window = 6\n",
    "        window = self.window\n",
    "\n",
    "        # Build maps\n",
    "        self.company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\n",
    "        self.date_to_index = {d:i for i,d in enumerate(date_universe)}\n",
    "        self.index_to_date = {i:d for i,d in enumerate(date_universe)}\n",
    "\n",
    "        # Store data\n",
    "        self.company_to_price_df = company_to_price_df\n",
    "        self.company_to_tweets = company_to_tweets\n",
    "        \n",
    "        # Get price data tensor: n_stocks, n_days, 3\n",
    "        self.price_data = np.zeros((n_stocks, n_days, 3))\n",
    "        for company in company_to_price_df.keys():\n",
    "            df = company_to_price_df[company]\n",
    "\n",
    "            # Look up specific rows in DF\n",
    "            for index, row in df.iterrows():\n",
    "\n",
    "                # Grab row with particular date\n",
    "                d_index = self.date_to_index[row['date']]\n",
    "                c_index = self.company_to_index[company]\n",
    "                self.price_data[c_index, d_index, 0] = row['high']\n",
    "                self.price_data[c_index, d_index, 1] = row['low']\n",
    "                self.price_data[c_index, d_index, 2] = row['adjust_close']\n",
    "                \n",
    "        # Which stocks are usable for these dates, shape n_days n_stocks\n",
    "        self.usable_stocks = torch.ones((self.n_days-7, self.n_stocks))\n",
    "    \n",
    "        # Labels of shape n_days, n_stocks\n",
    "        self.labels = torch.zeros((self.n_days-7, self.n_stocks))\n",
    "        \n",
    "\n",
    "        # Get labels\n",
    "        for i in range(self.n_days-7):\n",
    "            # Day after (for label)\n",
    "            day_after = self.index_to_date[i + window + 1]\n",
    "            # Current day\n",
    "            current_day = self.index_to_date[i + window]\n",
    "            for company in self.company_to_price_df.keys():\n",
    "                df = self.company_to_price_df[company]\n",
    "\n",
    "                # Grab row with particular date\n",
    "                post_row = df.loc[df['date'] == day_after]\n",
    "                row = df.loc[df['date'] == current_day]\n",
    "                c_index = self.company_to_index[company]\n",
    "\n",
    "                if (len(post_row['adjust_close']) > 0) and (len(row['adjust_close']) > 0):\n",
    "                    close = np.zeros((1))\n",
    "                    close[0] = post_row['adjust_close']\n",
    "                    close[0] /= row['adjust_close']\n",
    "                    if close >= 1.0055:\n",
    "                        self.labels[i, c_index] = 1\n",
    "                    elif close <= 0.995:\n",
    "                        self.labels[i, c_index] = 0\n",
    "                    else:\n",
    "                        self.usable_stocks[i, c_index] = 0\n",
    "                else:\n",
    "                    self.usable_stocks[i, c_index] = 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_days-7\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        gets a price tensor of shape (n_stocks, 6, 3)\n",
    "        gets a smi tensor of shape (n_stocks, 6, K, 512)\n",
    "        \"\"\"\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        # Size of sliding window\n",
    "        window = self.window\n",
    "        \n",
    "        # Current day's usable stocks from price filter\n",
    "        usable_stocks = self.usable_stocks[idx]\n",
    "        \n",
    "        # Labels from price day\n",
    "        labels = self.labels[idx]\n",
    "\n",
    "        # Dates that we need to look up\n",
    "        dates_range = [self.index_to_date[i] for i in range(idx + 1, idx + window + 1)]\n",
    "\n",
    "        # Day after (for label)\n",
    "        day_after = self.index_to_date[idx + window + 1]\n",
    "\n",
    "        # Current day\n",
    "        current_day = self.index_to_date[idx + window]\n",
    "\n",
    "        # Get price data tensor: n_stocks, window, 3\n",
    "        price_data = self.price_data[:, idx+1:idx+window+1, :]\n",
    "\n",
    "        # Extract tweets for specific window\n",
    "        smi_data = np.zeros((self.n_stocks, window, self.max_tweets, 512))\n",
    "        tweet_counts = np.zeros((self.n_stocks, window))\n",
    "        for company in self.company_to_tweets.keys():\n",
    "\n",
    "            # Look up tweets from specific days\n",
    "            for date_idx, date in enumerate(dates_range):\n",
    "                n_tweets = 0\n",
    "                tweets = []\n",
    "                c_index = self.company_to_index[company]\n",
    "                if date in self.company_to_tweets[company]:\n",
    "                    n_tweets = len(self.company_to_tweets[company][date])\n",
    "                    tweets = [self.company_to_tweets[company][date][k]['embedding'] for k in range(n_tweets)]\n",
    "                else:\n",
    "                    usable_stocks[c_index] = 0\n",
    "                tweet_counts[c_index, date_idx] = n_tweets\n",
    "                if n_tweets == 0:\n",
    "                    usable_stocks[c_index] = 0\n",
    "                for i,embedding in enumerate(tweets): \n",
    "                    #stocks, day, lags, tweet, embedding\n",
    "                    smi_data[c_index, date_idx, i, :] = embedding[:]\n",
    "\n",
    "        usable_stocks = (usable_stocks == 1)\n",
    "\n",
    "        m_mask = torch.zeros(6, self.n_stocks, self.max_tweets, 1)\n",
    "        for t in range(6):\n",
    "            for i in range(self.n_stocks):\n",
    "                m_mask[t, i, 0:int(round(tweet_counts[i][t]+1)), 0] = 1\n",
    "\n",
    "        price_output = price_data[usable_stocks,:,:]\n",
    "        smi_output = smi_data[usable_stocks,:,:,:]\n",
    "        tweet_count = tweet_counts[usable_stocks,:]\n",
    "        m_mask = m_mask[:,usable_stocks,:,:]\n",
    "        labels = labels[usable_stocks]\n",
    "        \n",
    "        # construct output\n",
    "        return price_output, smi_output, tweet_count, usable_stocks, labels, m_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "8W_Y3YxELDp6"
   },
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets)\n",
    "val_dataset = StockDataset(val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets)\n",
    "test_dataset = StockDataset(test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qV_GvReJLFM5"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlOeo8caLWKK"
   },
   "source": [
    "#Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANYU8fO-BdS6",
    "outputId": "8a8108c0-2e77-4b7e-ffee-2b12955a7925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzGtCej6Bu_n"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "etylmeyDBtQ4"
   },
   "outputs": [],
   "source": [
    "mansf = MANSF(T=6,\n",
    "              gru_hidden_size=64,\n",
    "              attn_inter_size=32,\n",
    "              use_embed_size=512,\n",
    "              blend_size=32,\n",
    "              gat_1_inter_size=32,\n",
    "              gat_2_inter_size=32,\n",
    "              leakyrelu_slope=0.01,\n",
    "              elu_alpha=1.0,\n",
    "              U=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "eNy0SuSXB0H2"
   },
   "outputs": [],
   "source": [
    "mansf = mansf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "ELS9f6JKCDCU"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mansf.parameters(), lr=5e-5)\n",
    "loss_fn = nn.BCELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "TeuL7Yt_shSW"
   },
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhxL1HqYCEPD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42b621463df54de99372c1486b0e989f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=392), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(18):\n",
    "    mansf.train()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    running_loss = 0.0\n",
    "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(train_dataloader):\n",
    "        price = price.type(torch.FloatTensor)\n",
    "        smi = smi.type(torch.FloatTensor)\n",
    "        price = price.float()\n",
    "        smi = smi.float()\n",
    "\n",
    "        price = price.to(device)\n",
    "        smi = smi.to(device)\n",
    "        n_tweets = n_tweets.to(device)\n",
    "        usable_stocks = usable_stocks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        m_mask = m_mask.to(device)\n",
    "\n",
    "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "        smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "        m = []\n",
    "        for t in range(6):\n",
    "            m.append(smi[t])\n",
    "\n",
    "        neighborhoods = torch.eye(87, 87)\n",
    "        neighborhoods = neighborhoods.to(device)\n",
    "        neighborhoods = neighborhoods[usable_stocks, :]\n",
    "        neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "        if price.shape[0] != 0:\n",
    "            y = mansf(price, smi, m_mask, neighborhoods)\n",
    "            loss = loss_fn(y.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            correct += torch.sum(((y > 0.5).view(-1) == labels.view(-1))).item()\n",
    "            total += len(y)\n",
    "            running_loss = loss.item() * len(y)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    mansf.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(val_dataloader):\n",
    "        price = price.type(torch.FloatTensor)\n",
    "        smi = smi.type(torch.FloatTensor)\n",
    "        price = price.float()\n",
    "        smi = smi.float()\n",
    "\n",
    "        price = price.to(device)\n",
    "        smi = smi.to(device)\n",
    "        n_tweets = n_tweets.to(device)\n",
    "        usable_stocks = usable_stocks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        m_mask = m_mask.to(device)\n",
    "\n",
    "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "        smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "        m = []\n",
    "        for t in range(6):\n",
    "            m.append(smi[t])\n",
    "\n",
    "        neighborhoods = torch.eye(87, 87)\n",
    "        neighborhoods = neighborhoods.to(device)\n",
    "        neighborhoods = neighborhoods[usable_stocks, :]\n",
    "        neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "        if price.shape[0] != 0:\n",
    "            y = mansf(price, smi, m_mask, neighborhoods)\n",
    "            #print(y)\n",
    "            correct += torch.sum((y > 0.5).view(-1) == labels.view(-1)).item()\n",
    "            total += len(y)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    print('epoch:', epoch, 'loss:', running_loss, 'train_acc:', train_acc, 'val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxKSr2BV4k_z"
   },
   "outputs": [],
   "source": [
    "mansf.eval()\n",
    "\n",
    "price, smi, n_tweets, usable_stocks, labels, m_mask = next(iter(val_dataloader))\n",
    "\n",
    "price = price.type(torch.FloatTensor)\n",
    "smi = smi.type(torch.FloatTensor)\n",
    "\n",
    "price = price.to(device)\n",
    "smi = smi.to(device)\n",
    "n_tweets = n_tweets.to(device)\n",
    "usable_stocks = usable_stocks.to(device)\n",
    "labels = labels.to(device)\n",
    "m_mask = m_mask.to(device)\n",
    "\n",
    "price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "m = []\n",
    "for t in range(6):\n",
    "    m.append(smi[t])\n",
    "\n",
    "neighborhoods = torch.eye(87, 87)\n",
    "neighborhoods = neighborhoods.to(device)\n",
    "neighborhoods = neighborhoods[usable_stocks, :]\n",
    "neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "y = mansf(price, smi, m_mask, neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqAGN_ND12G4",
    "outputId": "8a415adf-a839-4632-d3c4-32a8bd296471"
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2S1py5m47Ts",
    "outputId": "eb6cc1ee-369c-41b6-e019-c8b57d1e9148"
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64uHrKiQrq4X"
   },
   "source": [
    "#Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGV8hlffrr9k"
   },
   "outputs": [],
   "source": [
    "def plot(X, Y, xlabel, ylabel, legend, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        ax.plot(X, Y[i], label=legend[i])\n",
    "\n",
    "    plt.grid(color='0.95')\n",
    "    plt.legend()\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6pHZ4GIr1BI"
   },
   "outputs": [],
   "source": [
    "\"\"\"plot(range(18),\n",
    "     [train_acc_list, val_acc_list],\n",
    "     'epoch',\n",
    "     'accuracy',\n",
    "     ['training accuracy', 'validation accuracy'],\n",
    "     'accuracy vs. epoch')\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMkwD5KBuRvTKUZhZMRZdNb",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "testing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
