{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/96jonesa/CSE-517-Project/blob/main/testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qp0_axn_A0RP"
   },
   "source": [
    "#Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IouH5zC4C6tM"
   },
   "outputs": [],
   "source": [
    "!pip3 install --quiet \"tensorflow-hub>=0.7.0\"\n",
    "!pip3 install --quiet seaborn\n",
    "!pip3 install --quiet pandas-market-calendars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hkbW7y5PAL4E"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from absl import logging\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "import json\n",
    "import itertools\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pandas_market_calendars as mcal\n",
    "import datetime\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3_bjmyzAy67"
   },
   "source": [
    "#Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "V_I8XFDzAQaW"
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
    "        super(GRU, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_first = batch_first\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=self.batch_first)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output, hn = self.gru(input)\n",
    "        return output, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8pc_MLjpAQsN"
   },
   "outputs": [],
   "source": [
    "# attention weights are softmax(u^T tanh(W input + b)) where W is learned parameter matrix, u is a learned parameter vector, and b is a learned offset\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size, weights_size):\n",
    "        super(LinearAttention, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.weights_size = weights_size\n",
    "\n",
    "        self.linear_1 = nn.Linear(self.input_size, self.intermediate_size, bias=True)\n",
    "        self.linear_2 = nn.Linear(self.intermediate_size, self.weights_size, bias=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "\n",
    "    def forward(self, input, mask=None):\n",
    "        intermediate = self.tanh(self.linear_1(input))\n",
    "        pre_attention = self.linear_2(intermediate)\n",
    "        if mask is not None:\n",
    "            zero_vec = -9e15*torch.ones_like(pre_attention)\n",
    "            pre_attention = torch.where(mask > 0, pre_attention, zero_vec)\n",
    "        attention_weights = self.softmax(pre_attention)\n",
    "        attention_weights = attention_weights.permute(0, 2, 1)\n",
    "        output_features = torch.bmm(attention_weights, input)\n",
    "\n",
    "        return output_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "eZ4wJCUJATJ5"
   },
   "outputs": [],
   "source": [
    "# output is ReLU(left^T W right + b) where W is a learned paramater matrix\n",
    "# and b is a learned bias\n",
    "\n",
    "class Blend(nn.Module):\n",
    "    def __init__(self, left_size, right_size, output_size):\n",
    "        super(Blend, self).__init__()\n",
    "        self.left_size = left_size\n",
    "        self.right_size = right_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.bilinear = nn.Bilinear(self.left_size, self.right_size, output_size, bias=True)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, left, right):\n",
    "        output = self.relu(self.bilinear(left, right))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "UOSXvN33Ae-Q"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'class SGAT(nn.Module):\\n    def __init__(self, input_size, output_size, leakyrelu_slope=0.01):\\n        super(SGAT, self).__init__()\\n        self.input_size = input_size\\n        self.output_size = output_size\\n        self.leakyrelu_slope = leakyrelu_slope\\n        \\n        self.W = nn.Parameter(torch.empty(size=(input_size, output_size)))\\n        nn.init.xavier_uniform_(self.W.data, gain=1.414)\\n        self.a = nn.Parameter(torch.empty(size=(2*output_size, 1)))\\n        nn.init.xavier_uniform_(self.a.data, gain=1.414)\\n        self.leakyrelu = nn.LeakyReLU(self.leakyrelu_slope)\\n\\n    def forward(self, h, adj):\\n        Wh = torch.mm(h, self.W)\\n        a_input = self._prepare_attentional_mechanism_input(Wh)\\n        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\\n\\n        zero_vec = -9e15*torch.ones_like(e)\\n        attention = torch.where(adj > 0, e, zero_vec)\\n        attention = F.softmax(attention, dim=1)\\n        h_prime = torch.matmul(attention, Wh)\\n\\n        return h_prime\\n\\n    def _prepare_attentional_mechanism_input(self, Wh):\\n        N = Wh.size()[0] # number of nodes\\n        \\n        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\\n        Wh_repeated_alternating = Wh.repeat(N, 1)\\n\\n        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\\n\\n        return all_combinations_matrix.view(N, N, 2 * self.output_size)'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://github.com/Diego999/pyGAT/blob/master/layers.py\n",
    "\n",
    "\"\"\"class SGAT(nn.Module):\n",
    "    def __init__(self, input_size, output_size, leakyrelu_slope=0.01):\n",
    "        super(SGAT, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.leakyrelu_slope = leakyrelu_slope\n",
    "        \n",
    "        self.W = nn.Parameter(torch.empty(size=(input_size, output_size)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*output_size, 1)))\n",
    "        nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.leakyrelu_slope)\n",
    "\n",
    "    def forward(self, h, adj):\n",
    "        Wh = torch.mm(h, self.W)\n",
    "        a_input = self._prepare_attentional_mechanism_input(Wh)\n",
    "        e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "\n",
    "        zero_vec = -9e15*torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "\n",
    "        return h_prime\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, Wh):\n",
    "        N = Wh.size()[0] # number of nodes\n",
    "        \n",
    "        Wh_repeated_in_chunks = Wh.repeat_interleave(N, dim=0)\n",
    "        Wh_repeated_alternating = Wh.repeat(N, 1)\n",
    "\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "\n",
    "        return all_combinations_matrix.view(N, N, 2 * self.output_size)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "SgbnNp3nAh47"
   },
   "outputs": [],
   "source": [
    "class MANSF(nn.Module):\n",
    "    def __init__(self, T, gru_hidden_size, attn_inter_size, use_embed_size,\n",
    "                 blend_size, gat_1_inter_size, gat_2_inter_size, leakyrelu_slope, elu_alpha, U):\n",
    "        super(MANSF, self).__init__()\n",
    "        self.T = T\n",
    "        self.gru_hidden_size = gru_hidden_size\n",
    "        self.attn_inter_size = attn_inter_size\n",
    "        self.use_embed_size = use_embed_size\n",
    "        self.blend_size = blend_size\n",
    "        #self.gat_1_inter_size = gat_1_inter_size\n",
    "        #self.gat_2_inter_size = gat_2_inter_size\n",
    "        self.leakyrelu_slope = leakyrelu_slope\n",
    "        self.elu_alpha = elu_alpha\n",
    "        self.U = U\n",
    "\n",
    "        self.gru_p = GRU(3, gru_hidden_size, batch_first=True)\n",
    "        self.gru_m = GRU(use_embed_size, gru_hidden_size, batch_first=True)\n",
    "        self.gru_s = GRU(gru_hidden_size, gru_hidden_size, batch_first=True)\n",
    "        #self.attn_p = LinearAttention(gru_hidden_size, attn_inter_size, 1)\n",
    "        #self.attn_m = LinearAttention(gru_hidden_size, attn_inter_size, 1)\n",
    "        #self.attn_s = LinearAttention(gru_hidden_size, attn_inter_size, 1)\n",
    "        self.blend = Blend(gru_hidden_size, gru_hidden_size, blend_size)\n",
    "        #self.mgat_1 = nn.ModuleList([SGAT(blend_size, gat_1_inter_size, leakyrelu_slope=leakyrelu_slope) for u in range(U)])\n",
    "        #self.mgat_2 = nn.ModuleList([SGAT(U * gat_1_inter_size, gat_2_inter_size, leakyrelu_slope=leakyrelu_slope) for u in range(U)])\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.elu = nn.ELU(elu_alpha)\n",
    "        #self.final_linear = nn.Linear(U * gat_2_inter_size, 1, bias=True)\n",
    "        self.final_linear = nn.Linear(blend_size, 1, bias=True)\n",
    "\n",
    "    # p is price data tensor of shape (num_stocks, T, 3), for the day under consideration\n",
    "    # m is smi data list of tensors of shape (num_stocks, K, use_embed_size) of length T,\n",
    "    #       where K is the number of tweets for the given stock on the day under consideration\n",
    "    # neighorhoods is a list of adjacency lists, where each stock is indexed with the same\n",
    "    #       indices they have in p and m\n",
    "    def forward(self, p, m, m_mask, neighborhoods):\n",
    "        ## price encoding\n",
    "        #h_p, _ = self.gru_p(p)\n",
    "        _, h_p = self.gru_p(p)\n",
    "        #q = self.attn_p(h_p)\n",
    "\n",
    "        ## smi encoding (day level)\n",
    "        r = torch.zeros(p.shape[0], 0, self.gru_hidden_size)\n",
    "        r = r.to(device)\n",
    "        for t in range(self.T):\n",
    "            h_m, _ = self.gru_m(m[t])\n",
    "            #r_t = self.attn_m(h_m, m_mask[t])\n",
    "            r = torch.cat((r, h_m), 1)\n",
    "\n",
    "        ## smi encoding (aggregate)\n",
    "        #h_s, _ = self.gru_s(r)\n",
    "        _, h_s = self.gru_s(r)\n",
    "        #c = self.attn_s(h_s)\n",
    "\n",
    "        ## blending\n",
    "        #x = self.blend(q, c)\n",
    "        x = self.blend(h_p, h_s)\n",
    "\n",
    "        ## reshaping (eliminating superfluous dimension)\n",
    "        #x = x.view(x.shape[0], x.shape[2])\n",
    "        x = x.squeeze()\n",
    "\n",
    "        ## first gat layer\n",
    "        #  first head\n",
    "        \"\"\"sgat = self.mgat_1[0]\n",
    "        z = sgat(x, neighborhoods)\n",
    "        z = self.elu(z)\n",
    "\n",
    "        #  remaining heads\n",
    "        for u in range(1, self.U):\n",
    "            sgat = self.mgat_1[u]\n",
    "            z_u = sgat(x, neighborhoods)\n",
    "            z_u = self.elu(z_u)\n",
    "            \n",
    "            z = torch.cat((z, z_u), 1)\n",
    "        \n",
    "        ## second gat layer\n",
    "        #  first head\n",
    "        sgat = self.mgat_2[0]\n",
    "        new_z = sgat(z, neighborhoods)\n",
    "        new_z = self.sigmoid(new_z)\n",
    "\n",
    "        #  remaining heads\n",
    "        for u in range(1, self.U):\n",
    "            sgat = self.mgat_2[u]\n",
    "            new_z_u = sgat(z, neighborhoods)\n",
    "            new_z_u = self.sigmoid(new_z_u)\n",
    "            \n",
    "            new_z = torch.cat((new_z, new_z_u), 1)\"\"\"\n",
    "        \n",
    "        ## final layer\n",
    "        #y = self.sigmoid(self.final_linear(new_z))\n",
    "        y = self.sigmoid(self.final_linear(x))\n",
    "\n",
    "        ## return result\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyyPL-psA4cU"
   },
   "source": [
    "#Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "JZhP9VnQCyZm"
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/yumoxu/stocknet-dataset/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "aW75dPMQCzQ_"
   },
   "outputs": [],
   "source": [
    "#!unzip master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "xqxlViAXArti"
   },
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/2\", \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DRdIAHpeBJvw"
   },
   "outputs": [],
   "source": [
    "tf.disable_v2_behavior()\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "uK1ab2p7BLDm"
   },
   "outputs": [],
   "source": [
    "stocknet_dataset_filepath = './stocknet-dataset-master'\n",
    "train_start_date = '2014-01-01'\n",
    "train_end_date = '2015-07-31'\n",
    "val_start_date = '2015-08-01'\n",
    "val_end_date = '2015-09-30'\n",
    "test_start_date = '2015-10-01'\n",
    "test_end_date = '2016-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "oIRBbhAZy9ls"
   },
   "outputs": [],
   "source": [
    "def prep_dataset(dataset_filepath, start_date, end_date):\n",
    "    cache = {}\n",
    "    calendar = mcal.get_calendar('NYSE')\n",
    "    def next_trading_day(start_day=None, SAFE_DELTA = 4):\n",
    "        \"\"\"Returns the next/previous trading date separated by a certain number of \n",
    "        trading days.\n",
    "        \"\"\"\n",
    "        if start_day is None:\n",
    "            start_day = datetime.datetime.utcnow().date()\n",
    "        if start_day in cache:\n",
    "            return cache[start_day]\n",
    "        start = pd.to_datetime(start_day)\n",
    "        end = start + np.timedelta64(SAFE_DELTA, 'D')\n",
    "        business_days = calendar.valid_days(start_date=start, end_date=end)\n",
    "        next_day = business_days[1].date()\n",
    "        next_day = next_day.strftime(\"%Y-%m-%d\")\n",
    "        cache[start_day] = next_day\n",
    "        return next_day\n",
    "    \n",
    "    raw_prices_filepath = stocknet_dataset_filepath + '/price/raw'\n",
    "    preprocessed_tweets_filepath = stocknet_dataset_filepath + '/tweet/preprocessed'\n",
    "\n",
    "    company_to_price_df = {}\n",
    "    company_to_tweets = {}\n",
    "\n",
    "    for filename in os.listdir(raw_prices_filepath):\n",
    "        with open(raw_prices_filepath + '/' + filename) as file:\n",
    "            company_name = filename.split('.')[0]\n",
    "            \n",
    "            # Not enough data for GMRE\n",
    "            if company_name == 'GMRE':\n",
    "                continue\n",
    "            df = pd.read_csv(file)\n",
    "            df.columns = ['date', 'open', 'high', 'low', 'close', 'adjust_close', 'volume']\n",
    "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
    "            df = df.loc[mask]\n",
    "            company_to_price_df[company_name] = df.dropna()\n",
    "\n",
    "    for filename in tqdm(os.listdir(preprocessed_tweets_filepath)):\n",
    "        company_name = filename.split('.')[0]\n",
    "        dates_to_tweets = {}\n",
    "        for tweet_filename in os.listdir(preprocessed_tweets_filepath + '/' + filename):\n",
    "            if tweet_filename < start_date or tweet_filename > end_date:\n",
    "                continue\n",
    "            with open(preprocessed_tweets_filepath + '/' + filename + '/' + tweet_filename) as file:\n",
    "                list_of_tweets = []\n",
    "                for line in file:\n",
    "                    tweet_json = json.loads(line)\n",
    "                    list_of_tweets.append(tweet_json)\n",
    "                date_idx = next_trading_day(tweet_filename)\n",
    "                if date_idx not in dates_to_tweets:\n",
    "                    dates_to_tweets[date_idx] = list_of_tweets\n",
    "                else:\n",
    "                    dates_to_tweets[date_idx] += list_of_tweets\n",
    "        company_to_tweets[company_name] = dates_to_tweets\n",
    "    \n",
    "    # Reduce logging output.\n",
    "    logging.set_verbosity(logging.ERROR)\n",
    "    tf.get_logger().setLevel(logging.ERROR)\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "    # Import the Universal Sentence Encoder's TF Hub module\n",
    "    def embed_useT(module):\n",
    "        with tf.Graph().as_default():\n",
    "            sentences = tf.placeholder(tf.string)\n",
    "            embed = hub.Module(module)\n",
    "            embeddings = embed(sentences)\n",
    "            session = tf.train.MonitoredSession()\n",
    "        return lambda x: session.run(embeddings, {sentences: x})\n",
    "    embed_fn = embed_useT(module_url)\n",
    "\n",
    "    # Generate embeddings\n",
    "    for company in tqdm(company_to_tweets.keys()):\n",
    "        for date in company_to_tweets[company].keys():\n",
    "            messages = []\n",
    "            for j in range(len(company_to_tweets[company][date])):\n",
    "                messages.append(' '.join(company_to_tweets[company][date][j]['text']))\n",
    "                message_embeddings = embed_fn(messages)\n",
    "            for k in range(len(company_to_tweets[company][date])):\n",
    "                company_to_tweets[company][date][k]['embedding'] = list(message_embeddings[k])\n",
    "    \n",
    "    # Create date mapping\n",
    "    date_universe = set()\n",
    "    for company in company_to_price_df.keys():\n",
    "        date_universe = date_universe.union(set(company_to_price_df[company].date))\n",
    "    for company in company_to_tweets.keys():\n",
    "        date_universe = date_universe.union(set(company_to_tweets[company].keys()))\n",
    "    date_universe = sorted(list(date_universe))\n",
    "    index_to_date = {i-5:d for i,d in enumerate(date_universe)}\n",
    "    date_to_index = {d:i-5 for i,d in enumerate(date_universe)}\n",
    "\n",
    "    # Calculate dimensions for tensor\n",
    "    n_stocks = len(company_to_tweets.keys())\n",
    "    n_days = len(date_universe)\n",
    "    max_tweets = 0\n",
    "    for c,d in itertools.product(company_to_tweets.keys(), date_universe):\n",
    "        if d in company_to_tweets[c]:\n",
    "            max_tweets = max(max_tweets, len(company_to_tweets[c][d]))\n",
    "    # Create index mapping for stocks alphabetically\n",
    "    company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\n",
    "\n",
    "    return company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "7VwHfOnq19zW"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2ccd56430544a79cb3bf3bb9d02991",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd9ec9cc613b43a087dcfa103c9e4cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4bd802e6d3748db8ff7aa0278c9044e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f95c36bf35487e9ea17dce8a082b8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9882931fb48e4f3da2d23a7f3dfdfe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa1ccd9c17340349e41a49d232dd6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets = prep_dataset(stocknet_dataset_filepath, train_start_date, train_end_date)\n",
    "val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets = prep_dataset(stocknet_dataset_filepath, val_start_date, val_end_date)\n",
    "test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets = prep_dataset(stocknet_dataset_filepath, test_start_date, test_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td7nHz4py1wv"
   },
   "source": [
    "#Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "8XrZt8s1Kys5"
   },
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    \"\"\"Price dataset\"\"\"\n",
    "\n",
    "    def __init__(self, company_to_price_df, company_to_tweets, date_universe, n_days, n_stocks, max_tweets):\n",
    "        # Initialize class members\n",
    "        self.n_stocks = n_stocks\n",
    "        self.n_days = n_days\n",
    "        self.max_tweets = max_tweets\n",
    "\n",
    "        # Build maps\n",
    "        self.company_to_index = {c:i for i,c in enumerate(sorted(list(company_to_tweets.keys())))}\n",
    "        self.date_to_index = {d:i for i,d in enumerate(date_universe)}\n",
    "        self.index_to_date = {i:d for i,d in enumerate(date_universe)}\n",
    "\n",
    "        # Store data\n",
    "        self.company_to_price_df = company_to_price_df\n",
    "        self.company_to_tweets = company_to_tweets\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_days-7\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        gets a price tensor of shape (n_stocks, 6, 3)\n",
    "        gets a smi tensor of shape (n_stocks, 6, K, 512)\n",
    "        \"\"\"\n",
    "        # Size of sliding window\n",
    "        window = 6\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Dates that we need to look up\n",
    "        dates_range = [self.index_to_date[i] for i in range(idx + 1, idx + window + 1)]\n",
    "\n",
    "        # Day after (for label)\n",
    "        day_after = self.index_to_date[idx + window + 1]\n",
    "\n",
    "        # Current day\n",
    "        current_day = self.index_to_date[idx + window]\n",
    "\n",
    "        # Which stocks are usable for these dates\n",
    "        usable_stocks = torch.ones(self.n_stocks)\n",
    "\n",
    "        # Labels\n",
    "        labels = torch.zeros(self.n_stocks)\n",
    "\n",
    "        # Get labels\n",
    "        for company in self.company_to_price_df.keys():\n",
    "            df = self.company_to_price_df[company]\n",
    "\n",
    "            # Grab row with particular date\n",
    "            post_row = df.loc[df['date'] == day_after]\n",
    "            row = df.loc[df['date'] == current_day]\n",
    "            c_index = self.company_to_index[company]\n",
    "\n",
    "            if (len(post_row['adjust_close']) > 0) and (len(row['adjust_close']) > 0):\n",
    "                close = np.zeros((1))\n",
    "                close[0] = post_row['adjust_close']\n",
    "                close[0] /= row['adjust_close']\n",
    "                if close >= 1.0055:\n",
    "                    labels[c_index] = 1\n",
    "                elif close <= 0.995:\n",
    "                    labels[c_index] = 0\n",
    "                else:\n",
    "                    usable_stocks[c_index] = 0\n",
    "            else:\n",
    "                usable_stocks[c_index] = 0\n",
    "\n",
    "        # Get price data tensor: n_stocks, window, 3\n",
    "        price_data = np.zeros((self.n_stocks, window, 3))\n",
    "        for company in self.company_to_price_df.keys():\n",
    "            df = self.company_to_price_df[company]\n",
    "\n",
    "            prev_day = self.index_to_date[idx]\n",
    "            prev_row = df.loc[df['date'] == prev_day]\n",
    "            # Look up specific rows in DF\n",
    "            for date_idx, date in enumerate(dates_range):\n",
    "\n",
    "                # Grab row with particular date\n",
    "                row = df.loc[df['date'] == date]\n",
    "                c_index = self.company_to_index[company]\n",
    "\n",
    "                if (len(row['high']) > 0) and (len(row['low']) > 0) and (len(row['adjust_close']) > 0) and (len(prev_row['adjust_close']) > 0):\n",
    "                    price_data[c_index, date_idx, 0] = row['high']\n",
    "                    price_data[c_index, date_idx, 1] = row['low']\n",
    "                    price_data[c_index, date_idx, 2] = row['adjust_close']\n",
    "\n",
    "                    price_data[c_index, date_idx, 0] /= prev_row['adjust_close']\n",
    "                    price_data[c_index, date_idx, 1] /= prev_row['adjust_close']\n",
    "                    price_data[c_index, date_idx, 2] /= prev_row['adjust_close']\n",
    "                else:\n",
    "                    usable_stocks[c_index] = 0\n",
    "                \n",
    "                prev_row = row\n",
    "\n",
    "        # Extract tweets for specific window\n",
    "        smi_data = np.zeros((self.n_stocks, window, self.max_tweets, 512))\n",
    "        tweet_counts = np.zeros((self.n_stocks, window))\n",
    "        for company in self.company_to_tweets.keys():\n",
    "\n",
    "            # Look up tweets from specific days\n",
    "            for date_idx, date in enumerate(dates_range):\n",
    "                n_tweets = 0\n",
    "                tweets = []\n",
    "                c_index = self.company_to_index[company]\n",
    "                if date in self.company_to_tweets[company]:\n",
    "                    n_tweets = len(self.company_to_tweets[company][date])\n",
    "                    tweets = [self.company_to_tweets[company][date][k]['embedding'] for k in range(n_tweets)]\n",
    "                else:\n",
    "                    usable_stocks[c_index] = 0\n",
    "                tweet_counts[c_index, date_idx] = n_tweets\n",
    "                if n_tweets == 0:\n",
    "                    usable_stocks[c_index] = 0\n",
    "                for i,embedding in enumerate(tweets): \n",
    "                    #stocks, day, lags, tweet, embedding\n",
    "                    smi_data[c_index, date_idx, i, :] = embedding[:]\n",
    "\n",
    "        usable_stocks = (usable_stocks == 1)\n",
    "\n",
    "        m_mask = torch.zeros(6, self.n_stocks, self.max_tweets, 1)\n",
    "        for t in range(6):\n",
    "            for i in range(self.n_stocks):\n",
    "                for k in range(self.max_tweets):\n",
    "                    if k <= tweet_counts[i][t]:\n",
    "                        m_mask[t][i][k][0] = 1\n",
    "\n",
    "        #price_output = price_data[usable_stocks,:,:]\n",
    "        #smi_output = smi_data[usable_stocks,:,:,:]\n",
    "        #tweet_count = tweet_counts[usable_stocks,:]\n",
    "        price_output = torch.tensor(price_data[usable_stocks,:,:])\n",
    "        smi_output = torch.tensor(smi_data[usable_stocks,:,:,:])\n",
    "        tweet_count = torch.tensor(tweet_counts[usable_stocks,:])\n",
    "        m_mask = m_mask[:,usable_stocks,:,:]\n",
    "        labels = labels[usable_stocks]\n",
    "        \n",
    "        # construct output\n",
    "        return price_output, smi_output, tweet_count, usable_stocks, labels, m_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "8W_Y3YxELDp6"
   },
   "outputs": [],
   "source": [
    "train_dataset = StockDataset(train_company_to_price_df, train_company_to_tweets, train_date_universe, train_n_days, train_n_stocks, train_max_tweets)\n",
    "val_dataset = StockDataset(val_company_to_price_df, val_company_to_tweets, val_date_universe, val_n_days, val_n_stocks, val_max_tweets)\n",
    "test_dataset = StockDataset(test_company_to_price_df, test_company_to_tweets, test_date_universe, test_n_days, test_n_stocks, test_max_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "qV_GvReJLFM5"
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=1,\n",
    "                        shuffle=False, num_workers=0)\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1,\n",
    "                        shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tlOeo8caLWKK"
   },
   "source": [
    "#Separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ANYU8fO-BdS6",
    "outputId": "8a8108c0-2e77-4b7e-ffee-2b12955a7925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tzGtCej6Bu_n"
   },
   "source": [
    "#Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "etylmeyDBtQ4"
   },
   "outputs": [],
   "source": [
    "mansf = MANSF(T=6,\n",
    "              gru_hidden_size=64,\n",
    "              attn_inter_size=32,\n",
    "              use_embed_size=512,\n",
    "              blend_size=32,\n",
    "              gat_1_inter_size=32,\n",
    "              gat_2_inter_size=32,\n",
    "              leakyrelu_slope=0.01,\n",
    "              elu_alpha=1.0,\n",
    "              U=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "eNy0SuSXB0H2"
   },
   "outputs": [],
   "source": [
    "mansf = mansf.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "ELS9f6JKCDCU"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(mansf.parameters(), lr=5e-4)\n",
    "loss_fn = nn.BCELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "TeuL7Yt_shSW"
   },
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhxL1HqYCEPD"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2026d61474d439b894b0fcbcfc87c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=392), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434e26f36b9e4613a53ae5312a261310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5416],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5419]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5419]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5419],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5416],\n",
      "        [0.5415],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5419],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5416],\n",
      "        [0.5414],\n",
      "        [0.5414],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5414],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5419],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5415],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5415],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5419]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5419]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5716],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5417]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5418],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "tensor([[0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5415],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5416],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5415],\n",
      "        [0.5416],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5417],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5418],\n",
      "        [0.5419],\n",
      "        [0.5418],\n",
      "        [0.5418]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "\n",
      "epoch: 0 loss: 3.0678683519363403 train_acc: 0.5346083788706739 val_acc: 0.5147058823529411\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "725a33b24a9d4040b9de4ebf745f19da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=392), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(18):\n",
    "    mansf.train()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    running_loss = 0.0\n",
    "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(train_dataloader):\n",
    "        price = price.type(torch.FloatTensor)\n",
    "        smi = smi.type(torch.FloatTensor)\n",
    "        price = price.float()\n",
    "        smi = smi.float()\n",
    "\n",
    "        price = price.to(device)\n",
    "        smi = smi.to(device)\n",
    "        n_tweets = n_tweets.to(device)\n",
    "        usable_stocks = usable_stocks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        m_mask = m_mask.to(device)\n",
    "\n",
    "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "        #print(smi)\n",
    "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "        smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "        m = []\n",
    "        for t in range(6):\n",
    "            m.append(smi[t])\n",
    "\n",
    "        neighborhoods = torch.eye(87, 87)\n",
    "        neighborhoods = neighborhoods.to(device)\n",
    "        neighborhoods = neighborhoods[usable_stocks, :]\n",
    "        neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "        if price.shape[0] != 0:\n",
    "            y = mansf(price, smi, m_mask, neighborhoods)\n",
    "            loss = loss_fn(y.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            correct += torch.sum(((y > 0.5).view(-1) == labels.view(-1))).item()\n",
    "            total += len(y)\n",
    "            running_loss = loss.item() * len(y)\n",
    "\n",
    "    train_acc = correct / total\n",
    "    train_acc_list.append(train_acc)\n",
    "\n",
    "    mansf.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for price, smi, n_tweets, usable_stocks, labels, m_mask in tqdm(val_dataloader):\n",
    "        price = price.type(torch.FloatTensor)\n",
    "        smi = smi.type(torch.FloatTensor)\n",
    "        price = price.float()\n",
    "        smi = smi.float()\n",
    "\n",
    "        price = price.to(device)\n",
    "        smi = smi.to(device)\n",
    "        n_tweets = n_tweets.to(device)\n",
    "        usable_stocks = usable_stocks.to(device)\n",
    "        labels = labels.to(device)\n",
    "        m_mask = m_mask.to(device)\n",
    "\n",
    "        price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "        smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "        n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "        usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "        m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "        smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "        m = []\n",
    "        for t in range(6):\n",
    "            m.append(smi[t])\n",
    "\n",
    "        neighborhoods = torch.eye(87, 87)\n",
    "        neighborhoods = neighborhoods.to(device)\n",
    "        neighborhoods = neighborhoods[usable_stocks, :]\n",
    "        neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "        if price.shape[0] != 0:\n",
    "            y = mansf(price, smi, m_mask, neighborhoods)\n",
    "            #print(y)\n",
    "            correct += torch.sum((y > 0.5).view(-1) == labels.view(-1)).item()\n",
    "            total += len(y)\n",
    "\n",
    "    val_acc = correct / total\n",
    "    val_acc_list.append(val_acc)\n",
    "\n",
    "    print('epoch:', epoch, 'loss:', running_loss, 'train_acc:', train_acc, 'val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cxKSr2BV4k_z"
   },
   "outputs": [],
   "source": [
    "mansf.eval()\n",
    "\n",
    "price, smi, n_tweets, usable_stocks, labels, m_mask = next(iter(val_dataloader))\n",
    "\n",
    "price = price.type(torch.FloatTensor)\n",
    "smi = smi.type(torch.FloatTensor)\n",
    "\n",
    "price = price.to(device)\n",
    "smi = smi.to(device)\n",
    "n_tweets = n_tweets.to(device)\n",
    "usable_stocks = usable_stocks.to(device)\n",
    "labels = labels.to(device)\n",
    "m_mask = m_mask.to(device)\n",
    "\n",
    "price = price.view(price.shape[1], price.shape[2], price.shape[3])\n",
    "smi = smi.view(smi.shape[1], smi.shape[2], smi.shape[3], smi.shape[4])\n",
    "n_tweets = n_tweets.view(n_tweets.shape[1], n_tweets.shape[2])\n",
    "usable_stocks = usable_stocks.view(usable_stocks.shape[1])\n",
    "m_mask = m_mask.view(m_mask.shape[1], m_mask.shape[2], m_mask.shape[3], m_mask.shape[4])\n",
    "\n",
    "smi = smi.permute(1, 0, 2, 3)\n",
    "\n",
    "m = []\n",
    "for t in range(6):\n",
    "    m.append(smi[t])\n",
    "\n",
    "neighborhoods = torch.eye(87, 87)\n",
    "neighborhoods = neighborhoods.to(device)\n",
    "neighborhoods = neighborhoods[usable_stocks, :]\n",
    "neighborhoods = neighborhoods[:, usable_stocks]\n",
    "\n",
    "y = mansf(price, smi, m_mask, neighborhoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XqAGN_ND12G4",
    "outputId": "8a415adf-a839-4632-d3c4-32a8bd296471"
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2S1py5m47Ts",
    "outputId": "eb6cc1ee-369c-41b6-e019-c8b57d1e9148"
   },
   "outputs": [],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64uHrKiQrq4X"
   },
   "source": [
    "#Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yGV8hlffrr9k"
   },
   "outputs": [],
   "source": [
    "def plot(X, Y, xlabel, ylabel, legend, title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    for i in range(len(Y)):\n",
    "        ax.plot(X, Y[i], label=legend[i])\n",
    "\n",
    "    plt.grid(color='0.95')\n",
    "    plt.legend()\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6pHZ4GIr1BI"
   },
   "outputs": [],
   "source": [
    "\"\"\"plot(range(18),\n",
    "     [train_acc_list, val_acc_list],\n",
    "     'epoch',\n",
    "     'accuracy',\n",
    "     ['training accuracy', 'validation accuracy'],\n",
    "     'accuracy vs. epoch')\"\"\""
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMkwD5KBuRvTKUZhZMRZdNb",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "testing.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
